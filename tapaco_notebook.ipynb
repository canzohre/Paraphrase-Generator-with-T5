{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fa87bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pwd' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq,Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import datasets as dt\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fb3146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "546a3d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/Lenovo/.cache/huggingface/datasets/csv/default-acc1a73b7981a46d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 67.20it/s]\n",
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/Lenovo/.cache/huggingface/datasets/csv/default-acc1a73b7981a46d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 95.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Text', 'Paraphrase'],\n",
       "        num_rows: 90\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Text', 'Paraphrase'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tapaco_dataset=load_dataset(\"csv\",data_files=\"tapaco_paraphrases_dataset_2.csv\")\n",
    "Tapaco_dataset\n",
    "dataset_train=Tapaco_dataset[\"train\"]\n",
    "dataset_train.shuffle()\n",
    "dataset=dataset_train.train_test_split(test_size=10)\n",
    "dataset    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95350338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My brother watches television.',\n",
       " 'What are you so nervous about?',\n",
       " 'By the way, Mike, please tell me how to get to your house.',\n",
       " 'What are you so worried about?',\n",
       " 'Spring is in the air.',\n",
       " 'What made you angry?',\n",
       " \"That's my money.\",\n",
       " 'I eat some cheese.',\n",
       " 'Spring will be here soon.',\n",
       " \"I'm not a madman.\",\n",
       " \"I don't even have time for reading.\",\n",
       " 'I have no time even for reading.',\n",
       " \"I'm not stupid.\",\n",
       " 'Do you know how to speak English?',\n",
       " 'Spring has arrived early.',\n",
       " 'Are you speaking English?',\n",
       " 'When the father came home, I watched TV.',\n",
       " 'She refuses to say more about it.',\n",
       " 'Can you not speak English?',\n",
       " 'Why is he sad?',\n",
       " \"When the cat's away, the mice do play.\",\n",
       " 'Where is the librarian?',\n",
       " 'What made you so angry?',\n",
       " 'Spring has come.',\n",
       " 'Why are you guys so angry?',\n",
       " 'You don’t need to apply in advance.',\n",
       " \"I'm not a moron.\",\n",
       " 'Why are you upset?',\n",
       " \"I'm not crazy.\",\n",
       " 'I owe her money.',\n",
       " 'Can you speak English?',\n",
       " \"I'm eating cheese.\",\n",
       " 'That which is easily acquired is easily lost.',\n",
       " 'Where is the money exchange counter?',\n",
       " 'Do you speak English fluently?',\n",
       " 'What made him mad?',\n",
       " 'This money is mine.',\n",
       " 'It is Monday today.',\n",
       " 'I owe him one.',\n",
       " 'Why are you flinching?',\n",
       " 'Spring came.',\n",
       " 'Spring is drawing near.',\n",
       " \"When the cat's away the mice will play.\",\n",
       " 'When there is no tigers in mountains, the monkey becomes the king.',\n",
       " 'Monsoon is coming.',\n",
       " \"You're not sick.\",\n",
       " 'Why is it that she looks so sad?',\n",
       " 'It is Monday.',\n",
       " 'This is a great place.',\n",
       " 'Why are you so nervous?',\n",
       " 'This place is all right.',\n",
       " 'Why are you angry?',\n",
       " 'Spring has come around.',\n",
       " 'Why is she angry?',\n",
       " \"It'll be painless.\",\n",
       " 'The more languages you know the more of a person you are.',\n",
       " \"I'll be here tomorrow.\",\n",
       " 'Why are you scared?',\n",
       " 'What made her angry?',\n",
       " 'Let me in, please.',\n",
       " 'Why does she look so sad?',\n",
       " \"I think that I'm just exhausted.\",\n",
       " 'It is springtime.',\n",
       " 'I love this store.',\n",
       " 'I like Occitan.',\n",
       " 'Ill-gotten gains never benefit anyone.',\n",
       " \"When the landlord's away, the tenants will play.\",\n",
       " 'Spring will soon come.',\n",
       " \"It won't hurt.\",\n",
       " 'This is the perfect place.',\n",
       " 'Ill-gotten never profit.',\n",
       " 'Spring will come soon.',\n",
       " 'Why is he angry?',\n",
       " 'Why are you freaking out?',\n",
       " 'At night all cats are grey.',\n",
       " \"I'm no fool.\",\n",
       " 'I eat cheese.',\n",
       " 'Why are you afraid?',\n",
       " 'Today is Monday.',\n",
       " 'What makes you so sad?',\n",
       " \"You aren't sick.\",\n",
       " 'I owe you money.',\n",
       " 'This place is fantastic.',\n",
       " \"I'm not insane.\",\n",
       " \"I'm not sick.\",\n",
       " 'I am indebted to him.',\n",
       " 'What made him so mad?',\n",
       " \"I think I'm just tired.\",\n",
       " 'My favorite fish is salmon.',\n",
       " \"I'm not the crazy one.\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][\"Paraphrase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff50dcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Lenovo\\.cache\\huggingface\\datasets\\csv\\default-acc1a73b7981a46d\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-0395256018f6c96c.arrow\n",
      "                                                  \r"
     ]
    }
   ],
   "source": [
    "model_checkpoint=\"t5-base\"\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "max_input_length=512 #512 ve 1024 arasında değişiyor maksimum izin verilen uzunluk , garanti olsun diye 512 yaptım.\n",
    "\n",
    "def tokenizer_function(example) :\n",
    " \n",
    "    model_inputs = tokenizer(example[\"Text\"], max_length=max_input_length, truncation=True)\n",
    "\n",
    "    outputs = tokenizer(example[\"Paraphrase\"], max_length=max_input_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = outputs[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset=dataset.map(tokenizer_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6755ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Text', 'Paraphrase', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 90\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Text', 'Paraphrase', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1a23c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36af4eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
    "batch_size = 5\n",
    "model_name = \"t5_Paraphrase_model_TapacoDataset\"\n",
    "model_dir = f\"./paraphrase_model/{model_name}\"\n",
    "\n",
    "args =  TFTrainingArguments(\n",
    "    model_dir,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge1\",\n",
    "    \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "007e127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
    "metric = dt.load_metric(\"rouge\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc03bce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_display(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_predictions= tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "                      \n",
    "    result = metric.compute(predictions=decoded_predictions, references=decoded_labels,\n",
    "                            use_stemmer=True)\n",
    "    return {k: v for k, v in result.items()}\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a6bbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model_init=AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device),\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer.to(device),\n",
    "    compute_metrics=metrics_display,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faaa0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa553a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\n",
    "model_dir=\"./paraphrase_model/\"\n",
    "trainer.save_model(model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n",
    "max_input_length = 512\n",
    "\n",
    "import locale\n",
    "print(locale.getpreferredencoding())\n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "locale.getpreferredencoding = getpreferredencoding #locale kodunu hazır buldum encoding hatası almıştım utf-8'e sabitlemek için kullandım\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47c36b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase_generator(sentence):\n",
    "  \n",
    "    input_sent = [sentence]\n",
    "    inputs = tokenizer(input_sent, max_length=max_input_length, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, num_beams=5, do_sample=True, max_length=512)\n",
    "    final_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    paraphrase = decoded_output[0]\n",
    "    print(\"paraphrase of this sentence is: \" , paraphrase)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
